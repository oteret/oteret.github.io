[{"content":"Work done at Google, in collaboration with Luca Versari and Alexander Mordvintsev.\nCan we create feedforward neural networks that can both encode a target image and self-replicating with and without variation? Yes\u0026hellip; after quite some observations\u0026hellip;\nWork published and presented at ALIFE 2021.\nSee the paper.\n","description":"Creating a geneaology of neural networks encoding flowers.","id":0,"section":"posts","tags":null,"title":"Recursively Fertile Self-replicating Neural Agents","uri":"https://oteret.github.io/posts/self-repl-nn/"},{"content":"Work done at Google, in collaboration with Alexander Mordvintsev, Eyvind Niklasson and Michael Levin.\nHijacking the behaviour of pretrained Neural CA with adversarial attacks.\nSee the Distill article.\n","description":"Reprogramming Neural CA to exibit novel behavior through adversarial attacks.","id":1,"section":"posts","tags":null,"title":"Adversarial Reprogramming of Neural Cellular Automata","uri":"https://oteret.github.io/posts/adversarial-ca/"},{"content":"Work done at Google, in collaboration with Alexander Mordvintsev, Eyvind Niklasson, Michael Levin and Sam Greydanus.\nEnd-to-end differentiable Neural CAs capable of self-classifying the MNIST digit they compose.\nSee the Distill article and the colab for reproducing all the experiments.\n","description":"End-to-end differentiable Neural CAs capable of self-classifying the MNIST digit they compose.","id":2,"section":"posts","tags":null,"title":"Self-classifying MNIST Digits","uri":"https://oteret.github.io/posts/mnist-ca/"},{"content":"Work done at Google, in collaboration with Eyvind Niklasson and Alexander Mordvintsev.\nMeta learning a learning protocol that can be used instead of SGD, Adam, etc, to train feed-forward neural networks. At least, in theory.\nSee the Arxiv preprint and the code base. The code base contains colabs that show how to use it!\nMy first first author work. Very fundamental in nature and small in scope. Still, it is 9 pages long.\n","description":"Meta-learning a learning rule for feed-forward neural networks that does not use gradients.","id":3,"section":"posts","tags":null,"title":"MPLP: Learning a Message Passing Learning Protocol","uri":"https://oteret.github.io/posts/mplp/"},{"content":"My name is Ettore Randazzo. I am a Software Engineer and Researcher at Google Research in ZÃ¼rich, Switzerland. Notable interests of mine are machine intelligence, complex artificial life, philosophy, ethics, logic and mathematics, gaming, music (including playing electric guitar and piano), and writing.\nThis website is currently research focused, but come back to it in the future as I plan to expand it as I am ready to share more of what I do.\n","description":"Who am I?","id":4,"section":"","tags":null,"title":"About","uri":"https://oteret.github.io/about/"},{"content":"Work done at Google, in collaboration with Alexander Mordvintsev, Eyvind Niklasson, and Michael Levin.\nSee the Distill article.\n","description":"A Distill article on end-to-end differentiable, self-organising cellular automata.","id":5,"section":"posts","tags":null,"title":"Growing Neural Cellular Automata","uri":"https://oteret.github.io/posts/growing-ca/"}]